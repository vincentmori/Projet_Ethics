{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d096b134",
   "metadata": {},
   "source": [
    "# Import et chargement des csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "12010352",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\vince\\OneDrive\\Bureau\\ING5\\Ethics\\ethics_proj_venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    pipeline\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoConfig\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a3703414",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_data = os.path.join(\"..\", \"Data\", \"daily_Sample.csv\")\n",
    "df = pd.read_csv(path_data, index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ba7f3eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_biais = os.path.join(\"..\", \"Data\", \"Daily_Biais_Sample.csv\")\n",
    "df_biais = pd.read_csv(path_biais, index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9127dddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_pred = [\n",
    "    \"intervention_diet_coaching\",\n",
    "    \"intervention_exercise_plan\",\n",
    "    \"intervention_meditation\",\n",
    "    \"intervention_sick_leave\",\n",
    "    \"intervention_therapy\",\n",
    "    \"intervention_vacation\",\n",
    "    \"intervention_workload_cap\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aa164d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_test = train_test_split(df, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daab7423",
   "metadata": {},
   "source": [
    "# Chargement du mod√®le pr√©-entrain√©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2a6dfe8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîµ Chargement du mod√®le BASELINE (non fine-tun√©)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at distilgpt2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Mod√®le baseline charg√© avec succ√®s\n",
      " ¬† Nombre de param√®tres : 81,917,952\n"
     ]
    }
   ],
   "source": [
    "model_name = \"distilgpt2\"\n",
    "N_LABELS = len(col_pred) \n",
    "\n",
    "# Chargement du tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Configuration importante pour GPT-2\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Chargement du mod√®le de base (AVANT fine-tuning)\n",
    "print(\"\\nüîµ Chargement du mod√®le BASELINE (non fine-tun√©)...\")\n",
    "model_baseline = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=N_LABELS, \n",
    "    problem_type=\"multi_label_classification\" \n",
    ")\n",
    "model_baseline.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "print(\"‚úÖ Mod√®le baseline charg√© avec succ√®s\")\n",
    "print(f\" ¬† Nombre de param√®tres : {model_baseline.num_parameters():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b2c117",
   "metadata": {},
   "source": [
    "# Mise en forme des datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6a3d645a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CODE A INSERER : Fonction de conversion\n",
    "def format_row_to_text(row):\n",
    "    # Impl√©mentation : concat√©ner toutes les colonnes de la ligne en une seule cha√Æne\n",
    "    return \"; \".join([f\"{col}: {value}\" for col, value in row.items()]) + \".\"\n",
    "\n",
    "# CR√âER LA COLONNE 'text' et 'labels' dans les DataFrames de travail\n",
    "df_train['text'] = df_train.drop(columns=col_pred).apply(format_row_to_text, axis=1)\n",
    "df_test['text'] = df_test.drop(columns=col_pred).apply(format_row_to_text, axis=1)\n",
    "\n",
    "# Cr√©er la colonne 'labels' pour les 7 interventions (obligatoire pour le Trainer)\n",
    "df_train['labels'] = df_train[col_pred].values.astype(np.float32).tolist()\n",
    "df_test['labels'] = df_test[col_pred].values.astype(np.float32).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6da55077",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28616"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d15b6d57",
   "metadata": {},
   "source": [
    "# Fine Tuning complet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7692f4b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 28616/28616 [00:05<00:00, 4904.30 examples/s]\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7154/7154 [00:01<00:00, 5333.82 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Datasets pr√©par√©s pour le fine-tuning\n",
      "\n",
      "üü¢ Initialisation d'un nouveau mod√®le pour le fine-tuning...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at distilgpt2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üöÄ D√©but du fine-tuning...\n",
      "   (Cela peut prendre quelques minutes selon votre machine)\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='712' max='10731' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  712/10731 43:48 < 10:18:11, 0.27 it/s, Epoch 0.20/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 1. Pr√©paration des datasets pour Hugging Face\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(\n",
    "        examples['text'],\n",
    "        truncation=True,\n",
    "        max_length=128,\n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "\n",
    "# 2. Conversion en Dataset Hugging Face\n",
    "train_dataset = Dataset.from_pandas(df_train)\n",
    "test_dataset = Dataset.from_pandas(df_test)\n",
    "\n",
    "# Tokenization\n",
    "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "test_dataset = test_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Format PyTorch\n",
    "train_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "test_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "\n",
    "print(f\"‚úÖ Datasets pr√©par√©s pour le fine-tuning\")\n",
    "\n",
    "N_LABELS=len(col_pred)\n",
    "# 3. Nouveau mod√®le pour le fine-tuning\n",
    "print(\"\\nüü¢ Initialisation d'un nouveau mod√®le pour le fine-tuning...\")\n",
    "model_finetuned = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=N_LABELS,\n",
    "    problem_type=\"multi_label_classification\"\n",
    ")\n",
    "model_finetuned.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "# 4. Configuration du training (corrig√©e)\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    warmup_steps=50,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    "    eval_strategy=\"epoch\",      # <-- anciennement evaluation_strategy\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    push_to_hub=False,\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "\n",
    "# 5. Fonction de calcul des m√©triques\n",
    "def compute_metrics_multilabel(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    probabilities = 1 / (1 + np.exp(-logits))\n",
    "    predictions = (probabilities >= 0.5).astype(int)\n",
    "    accuracy = accuracy_score(labels, predictions)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        labels, predictions, average='samples', zero_division=0\n",
    "    )\n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1\n",
    "    }\n",
    "\n",
    "# 6. Trainer\n",
    "trainer = Trainer(\n",
    "    model=model_finetuned,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    compute_metrics=compute_metrics_multilabel,\n",
    ")\n",
    "\n",
    "# 7. Fine-tuning\n",
    "print(\"\\nüöÄ D√©but du fine-tuning...\")\n",
    "print(\"   (Cela peut prendre quelques minutes selon votre machine)\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "train_result = trainer.train()\n",
    "\n",
    "print(\"\\n‚úÖ Fine-tuning termin√© !\")\n",
    "print(f\"   Loss finale : {train_result.training_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c1a50e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05b7cd25",
   "metadata": {},
   "source": [
    "# LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ed05a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Charger un mod√®le frais pour l'entra√Ænement LoRA\n",
    "print(\"\\\\nüîµ Initialisation d'un nouveau mod√®le pour le fine-tuning LoRA...\")\n",
    "model_lora = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=len(col_pred),\n",
    "    problem_type=\"multi_label_classification\"\n",
    ")\n",
    "model_lora.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "# 2. D√©finir la configuration de LoRA\n",
    "#    r: La dimension du rang (plus c'est petit, moins de param√®tres √† entra√Æner)\n",
    "#    lora_alpha: Le facteur d'√©chelle pour les poids LoRA\n",
    "#    target_modules: Les couches du mod√®le sur lesquelles appliquer LoRA. Pour GPT-2, 'c_attn' est la couche d'attention cl√©.\n",
    "#    task_type: Sp√©cifie la t√¢che pour configurer correctement le mod√®le\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\"c_attn\"],\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.SEQ_CLS\n",
    ")\n",
    "\n",
    "# 3. Appliquer LoRA au mod√®le\n",
    "print(\"   Application de la configuration LoRA au mod√®le...\")\n",
    "model_lora = get_peft_model(model_lora, lora_config)\n",
    "\n",
    "# Afficher le nombre de param√®tres entra√Ænables pour voir la diff√©rence !\n",
    "print(\"\\\\nüìä Comparaison des param√®tres entra√Ænables :\")\n",
    "def print_trainable_parameters(model, model_name=\"model\"):\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"   - {model_name}: {trainable_params:,} param√®tres entra√Ænables ({100 * trainable_params / all_param:.4f}%)\"\n",
    "    )\n",
    "\n",
    "print_trainable_parameters(model_finetuned, \"Mod√®le Complet\")\n",
    "print_trainable_parameters(model_lora, \"Mod√®le LoRA\")\n",
    "\n",
    "\n",
    "# 4. Entra√Æner avec le Trainer (le m√™me Trainer sait g√©rer un mod√®le PEFT)\n",
    "trainer_lora = Trainer(\n",
    "    model=model_lora,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    compute_metrics=compute_metrics_multilabel,\n",
    ")\n",
    "\n",
    "# 5. Lancer le fine-tuning LoRA\n",
    "print(\"\\\\nüöÄ D√©but du fine-tuning LoRA...\")\n",
    "print(\"-\" * 60)\n",
    "train_result_lora = trainer_lora.train()\n",
    "\n",
    "print(\"\\\\n‚úÖ Fine-tuning LoRA termin√© !\")\n",
    "print(f\"   Loss finale : {train_result_lora.training_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdac0204",
   "metadata": {},
   "source": [
    "# Distillation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "971b035e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Le \"professeur\" est notre mod√®le BASELINE (non fine-tun√©)\n",
    "teacher_model = model_baseline\n",
    "teacher_model.eval() # Mettre le professeur en mode √©valuation\n",
    "\n",
    "# 2. Cr√©er un mod√®le \"√©l√®ve\" plus petit\n",
    "#    On utilise la m√™me architecture (GPT-2) mais avec moins de couches et une taille cach√©e plus faible.\n",
    "print(\"\\nüîµ Cr√©ation du mod√®le √©l√®ve (student)...\")\n",
    "student_config = AutoConfig.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=len(col_pred),\n",
    "    problem_type=\"multi_label_classification\",\n",
    "    n_layer=3,  # distilgpt2 a 6 couches, on en met 3\n",
    "    n_head=4,   # distilgpt2 a 12 t√™tes, on en met 4\n",
    "    n_embd=256, # distilgpt2 a 768, on met 256\n",
    ")\n",
    "student_config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "student_model = AutoModelForSequenceClassification.from_config(student_config)\n",
    "\n",
    "print(\"\\nüìä Comparaison des tailles de mod√®les :\")\n",
    "print(f\"   - Professeur : {teacher_model.num_parameters():,} param√®tres\")\n",
    "print(f\"   - √âl√®ve      : {student_model.num_parameters():,} param√®tres\")\n",
    "\n",
    "\n",
    "# 3. Cr√©er un Trainer personnalis√© pour la distillation\n",
    "class DistillationTrainer(Trainer):\n",
    "    def __init__(self, *args, teacher_model=None, alpha=0.5, temperature=2.0, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.teacher_model = teacher_model\n",
    "        self.alpha = alpha\n",
    "        self.temperature = temperature\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        # Sorties de l'√©l√®ve\n",
    "        outputs_student = model(**inputs)\n",
    "        loss_ce = outputs_student.loss # Loss classique avec les vrais labels\n",
    "        logits_student = outputs_student.logits\n",
    "\n",
    "        # Sorties du professeur\n",
    "        with torch.no_grad():\n",
    "            outputs_teacher = self.teacher_model(**inputs)\n",
    "            logits_teacher = outputs_teacher.logits\n",
    "\n",
    "        # Calcul de la loss de distillation (KL Divergence)\n",
    "        loss_kd = F.kl_div(\n",
    "            F.log_softmax(logits_student / self.temperature, dim=-1),\n",
    "            F.softmax(logits_teacher / self.temperature, dim=-1),\n",
    "            reduction='batchmean'\n",
    "        ) * (self.temperature ** 2)\n",
    "\n",
    "        # Combinaison des deux losses\n",
    "        loss = self.alpha * loss_ce + (1 - self.alpha) * loss_kd\n",
    "\n",
    "        return (loss, outputs_student) if return_outputs else loss\n",
    "\n",
    "# 4. Instancier le DistillationTrainer\n",
    "#    alpha: poids de la loss classique (0.5 = 50% CE, 50% KD)\n",
    "#    temperature: \"adoucit\" les probabilit√©s du professeur pour donner plus d'infos √† l'√©l√®ve\n",
    "trainer_distill = DistillationTrainer(\n",
    "    model=student_model,\n",
    "    teacher_model=teacher_model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    compute_metrics=compute_metrics_multilabel,\n",
    "    alpha=0.5,\n",
    "    temperature=2.0,\n",
    ")\n",
    "\n",
    "# 5. Lancer l'entra√Ænement par distillation\n",
    "print(\"\\nüöÄ D√©but de l'entra√Ænement par distillation...\")\n",
    "print(\"-\" * 60)\n",
    "train_result_distill = trainer_distill.train()\n",
    "\n",
    "print(\"\\n‚úÖ Distillation termin√©e !\")\n",
    "print(f\"   Loss finale : {train_result_distill.training_loss:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ethics_proj_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
